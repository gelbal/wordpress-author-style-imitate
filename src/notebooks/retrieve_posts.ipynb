{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple, Any, Set\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.utils.helper import load_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "load_env()\n",
    "CLIENT_SECRET = os.getenv(\"WPCOM_CLIENT_SECRET\")\n",
    "ACCESS_TOKEN = os.getenv(\"WPCOM_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define WordPress.com API constants\n",
    "CLIENT_ID = '<CLIENT_ID>'\n",
    "REDIRECT_URI = '<REDIRECT_URI>'\n",
    "AUTHORIZATION_BASE_URL = 'https://public-api.wordpress.com/oauth2/authorize'\n",
    "TOKEN_URL = 'https://public-api.wordpress.com/oauth2/token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_file(filename: str, data: List[Dict]):\n",
    "    \"\"\"\n",
    "    Save data to a JSON file, appending if the file exists.\n",
    "    \"\"\"\n",
    "    existing_data = []\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "\n",
    "    # Combine existing data with new data, avoiding duplicates\n",
    "    existing_urls = set(post['URL'] for post in existing_data)\n",
    "    new_data = [post for post in data if post['URL'] not in existing_urls]\n",
    "    existing_data.extend(new_data)\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(existing_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def save_posts_to_disk(posts: List[Dict], site: str, usernames: List[str], output_dir: str = \"data\"):\n",
    "    \"\"\"\n",
    "    Save the downloaded posts to disk as JSON files, organized by domain and author.\n",
    "\n",
    "    Args:\n",
    "        posts (List[Dict]): A list of dictionaries containing post data.\n",
    "        site (str): The site domain.\n",
    "        usernames (List[str]): List of usernames to filter posts by.\n",
    "        output_dir (str): The name of the output directory.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "    domain_output_dir = os.path.join(project_root, output_dir, 'domain_posts')\n",
    "    author_output_dir = os.path.join(project_root, output_dir, 'author_posts')\n",
    "\n",
    "    os.makedirs(domain_output_dir, exist_ok=True)\n",
    "    os.makedirs(author_output_dir, exist_ok=True)\n",
    "\n",
    "    # Save all posts for the domain\n",
    "    domain_filename = os.path.join(domain_output_dir, f\"{site}.json\")\n",
    "    save_json_file(domain_filename, posts)\n",
    "    print(f\"All posts for site {site} saved to {domain_filename}\")\n",
    "\n",
    "    # Filter and save posts by author\n",
    "    for username in usernames:\n",
    "        author_posts = [post for post in posts if post['author_login'].lower() == username.lower()]\n",
    "        if author_posts:\n",
    "            author_filename = os.path.join(author_output_dir, f\"{username}.json\")\n",
    "            save_json_file(author_filename, author_posts)\n",
    "            print(f\"Posts for author {username} saved to {author_filename}\")\n",
    "\n",
    "    handle_cross_posts(posts, site, output_dir)\n",
    "\n",
    "def load_posts_from_disk(site: str, username: str = None, output_dir: str = \"data\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load posts from JSON files, either by domain or by author.\n",
    "\n",
    "    Args:\n",
    "        site (str): The site domain.\n",
    "        username (str, optional): The username to load posts for. If None, load all posts for the site.\n",
    "        output_dir (str): The name of the output directory.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of dictionaries containing post data.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "\n",
    "    if username:\n",
    "        filename = os.path.join(project_root, output_dir, 'author_posts', f\"{username}.json\")\n",
    "    else:\n",
    "        filename = os.path.join(project_root, output_dir, 'domain_posts', f\"{site}.json\")\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            posts = json.load(f)\n",
    "        print(f\"Loaded {len(posts)} posts from {filename}\")\n",
    "        return posts\n",
    "    else:\n",
    "        print(f\"No saved posts found for {'author ' + username if username else 'site ' + site}\")\n",
    "        return []\n",
    "\n",
    "def handle_cross_posts(posts: List[Dict], file_domain: str, output_dir: str = \"data\"):\n",
    "    \"\"\"\n",
    "    Write cross-posts to their own domain files.\n",
    "\n",
    "    Args:\n",
    "        posts (List[Dict]): List of posts to process.\n",
    "        file_domain (str): The domain of the current file being processed.\n",
    "        output_dir (str): The name of the output directory.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "    domain_output_dir = os.path.join(project_root, output_dir, 'domain_posts')\n",
    "\n",
    "    cross_posts = {}\n",
    "\n",
    "    for post in posts:\n",
    "        post_domain = urlparse(post['URL']).netloc\n",
    "        if post_domain != file_domain:\n",
    "            if post_domain not in cross_posts:\n",
    "                cross_posts[post_domain] = []\n",
    "            cross_posts[post_domain].append(post)\n",
    "\n",
    "    for domain, domain_posts in cross_posts.items():\n",
    "        cross_post_filename = os.path.join(domain_output_dir, f\"{domain}.json\")\n",
    "        save_json_file(cross_post_filename, domain_posts)\n",
    "        print(f\"Wrote {len(domain_posts)} cross-posts to {cross_post_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_limit(max_requests: int = 100, period: int = 10):\n",
    "    \"\"\"\n",
    "    Decorator to limit the rate of API calls.\n",
    "\n",
    "    Args:\n",
    "        max_requests (int): Maximum number of requests allowed in the given period.\n",
    "        period (int): Time period in seconds.\n",
    "    \"\"\"\n",
    "    calls: List[float] = []\n",
    "\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            now = time.time()\n",
    "            calls.append(now)\n",
    "            if len(calls) > max_requests:\n",
    "                oldest = calls.pop(0)\n",
    "                if now - oldest < period:\n",
    "                    time.sleep(period - (now - oldest))\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rate_limit()\n",
    "def get_post_data(site: str, post_id: int, access_token: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch metadata for a specific post.\n",
    "    \"\"\"\n",
    "    url = f\"https://public-api.wordpress.com/rest/v1.1/sites/{site}/posts/{post_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    params = {\n",
    "        \"fields\": \"ID,site_ID,author,date,title,URL,content,excerpt,status,like_count,comment_count,tags,categories\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        post = response.json()\n",
    "        processed_post = {\n",
    "            'post_ID': post['ID'],\n",
    "            'site_ID': post['site_ID'],\n",
    "            'site_domain': site,\n",
    "            'author_user_ID': post['author']['ID'],\n",
    "            'author_login': post['author']['login'],\n",
    "            'date': post['date'],\n",
    "            'title': post['title'],\n",
    "            'URL': post['URL'],\n",
    "            'content': post['content'],\n",
    "            'excerpt': post['excerpt'],\n",
    "            'status': post['status'],\n",
    "            'like_count': post['like_count'],\n",
    "            'tags': post.get('tags', {}),\n",
    "            'categories': post.get('categories', {})\n",
    "        }\n",
    "\n",
    "        # Only store the tags and categories as lists of names\n",
    "        processed_post['tags'] = extract_names(processed_post['tags'])\n",
    "        processed_post['categories'] = extract_names(processed_post['categories'])\n",
    "\n",
    "        # Fetch comments count\n",
    "        comments_url = f\"https://public-api.wordpress.com/rest/v1.1/sites/{site}/posts/{post['ID']}/replies/\"\n",
    "        comments_response = requests.get(comments_url, headers=headers)\n",
    "        if comments_response.status_code == 200:\n",
    "            comments_data = comments_response.json()\n",
    "            processed_post['comment_count'] = comments_data.get('found', 0)\n",
    "        else:\n",
    "            processed_post['comment_count'] = 0\n",
    "\n",
    "        # Fetch views count\n",
    "        views_url = f\"https://public-api.wordpress.com/rest/v1.1/sites/{site}/stats/post/{post['ID']}\"\n",
    "        views_response = requests.get(views_url, headers=headers)\n",
    "        if views_response.status_code == 200:\n",
    "            views_data = views_response.json()\n",
    "            processed_post['views'] = views_data.get('views', 0)\n",
    "        else:\n",
    "            processed_post['views'] = 0\n",
    "\n",
    "        return processed_post\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch post data for post ID {post_id} on site '{site}'. HTTP Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def extract_names(metadata_dict: Dict) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts the names of tags or categories from the metadata dictionary.\n",
    "    \"\"\"\n",
    "    return list(metadata_dict.keys())\n",
    "\n",
    "def extract_original_post_url(content: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the original post URL from X-post content.\n",
    "\n",
    "    Args:\n",
    "        content (str): The content of the X-post.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The URL of the original post if found, None otherwise.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    if links:\n",
    "        return links[-1]['href']\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_site_and_slug_from_url(url: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract site domain and post slug from a WordPress post URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the WordPress post.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: A tuple containing the site domain and post slug.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    site = parsed_url.netloc\n",
    "    path_parts = parsed_url.path.strip('/').split('/')\n",
    "\n",
    "    # Assuming the URL structure is like: https://<site>/YYYY/MM/DD/<post-slug>/\n",
    "    if len(path_parts) >= 4 and all(part.isdigit() for part in path_parts[:3]):\n",
    "        slug = path_parts[-1]\n",
    "    else:\n",
    "        # If the URL doesn't match the expected format, use the last part as slug\n",
    "        slug = path_parts[-1]\n",
    "\n",
    "    return site, slug\n",
    "\n",
    "\n",
    "@rate_limit()\n",
    "def get_post_by_url(url: str, access_token: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch post data by URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the WordPress post.\n",
    "        access_token (str): The access token for API authentication.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict]: A dictionary containing post data if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    site, slug = get_site_and_slug_from_url(url)\n",
    "    api_url = f\"https://public-api.wordpress.com/rest/v1.1/sites/{site}/posts/slug:{slug}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        post_data = response.json()\n",
    "        return post_data\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch post data for URL {url}. HTTP Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "@rate_limit()\n",
    "def fetch_new_posts(site: str, access_token: str, processed_urls: Set[str], start_date: datetime) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch new posts that haven't been processed before.\n",
    "\n",
    "    Args:\n",
    "        site (str): The WordPress site domain.\n",
    "        access_token (str): The access token for API authentication.\n",
    "        processed_urls (Set[str]): Set of already processed post URLs.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of dictionaries containing new post data.\n",
    "    \"\"\"\n",
    "    url = f\"https://public-api.wordpress.com/rest/v1.1/sites/{site}/posts/\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    params = {\n",
    "        'number': 100,\n",
    "        'fields': \"ID,site_ID,author,date,title,URL,content,excerpt,status,like_count,tags,categories\",\n",
    "        \"after\": start_date.isoformat()\n",
    "    }\n",
    "    all_posts = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        params['page'] = page\n",
    "        print(f\"Fetching page {page} of posts from site '{site}'\")\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            posts = data.get('posts', [])\n",
    "            for post in posts:\n",
    "                if post['URL'] in processed_urls:\n",
    "                    continue\n",
    "                print(f\"Processing post: {post['URL']}\")\n",
    "\n",
    "                if post['title'].startswith(\"X-post:\"):\n",
    "                    original_url = extract_original_post_url(post['content'])\n",
    "                    if original_url and original_url not in processed_urls:\n",
    "                        original_post = get_post_by_url(original_url, access_token)\n",
    "                        if original_post:\n",
    "                            processed_post = get_post_data(original_post['site_ID'], original_post['ID'], access_token)\n",
    "                            if processed_post:\n",
    "                                all_posts.append(processed_post)\n",
    "                                processed_urls.add(original_url)\n",
    "                else:\n",
    "                    processed_post = get_post_data(site, post['ID'], access_token)\n",
    "                    if processed_post:\n",
    "                        all_posts.append(processed_post)\n",
    "                        processed_urls.add(post['URL'])\n",
    "\n",
    "            if len(posts) < 100:  # Less than 100 posts returned, we've reached the end\n",
    "                break\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"Error: Unable to fetch posts for site '{site}'. HTTP Status Code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_posts_for_site(site: str, access_token: str, processed_urls: Set[str], usernames: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch all posts from a specific site, including previously processed posts.\n",
    "\n",
    "    Args:\n",
    "        site (str): The WordPress site domain.\n",
    "        access_token (str): The access token for API authentication.\n",
    "        processed_urls (Set[str]): Set of already processed post URLs.\n",
    "        usernames (List[str]): List of usernames to filter posts by.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of dictionaries containing post data.\n",
    "    \"\"\"\n",
    "    # Load previously processed posts\n",
    "    all_posts = load_posts_from_disk(site)\n",
    "\n",
    "    # Add URLs of loaded posts to processed_urls\n",
    "    processed_urls.update(post['URL'] for post in all_posts)\n",
    "\n",
    "    # Retrieve posts from the last 4 years. No need to go back further.\n",
    "    three_years_ago = datetime.now() - timedelta(days=3*365)\n",
    "\n",
    "    new_posts = fetch_new_posts(site, access_token, processed_urls, three_years_ago)\n",
    "    all_posts.extend(new_posts)\n",
    "\n",
    "    # Save all posts to disk\n",
    "    save_posts_to_disk(all_posts, site, usernames)\n",
    "\n",
    "    handle_cross_posts(all_posts, site)\n",
    "\n",
    "    return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\"site1.com\", \"site2.com\", \"site3.com\", \"site4.com\", \"site5.com\"]\n",
    "usernames = [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"]\n",
    "processed_urls = set()\n",
    "\n",
    "for site in sites:\n",
    "    posts = get_all_posts_for_site(site, ACCESS_TOKEN, processed_urls, usernames)\n",
    "    print(f\"Total posts for site {site}: {len(posts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordpress-style-imitate-rCnBc-L7-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
